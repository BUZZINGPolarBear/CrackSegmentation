diff --git a/.gitignore b/.gitignore
index 020c60a..14b02be 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,4 @@
 *.DS_Store
 .idea/
-image/
\ No newline at end of file
+image/
+*.gz
\ No newline at end of file
diff --git a/pytorch_lightning_first_try/main.ipynb b/pytorch_lightning_first_try/main.ipynb
index edccf37..58fccb5 100644
--- a/pytorch_lightning_first_try/main.ipynb
+++ b/pytorch_lightning_first_try/main.ipynb
@@ -127,175 +127,404 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 7,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Collecting lightning-bolts\n",
-      "  Downloading lightning_bolts-0.5.0-py3-none-any.whl (316 kB)\n",
-      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.8/316.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
-      "\u001b[?25hRequirement already satisfied: pytorch-lightning>=1.4.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from lightning-bolts) (1.7.7)\n",
-      "Requirement already satisfied: packaging in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from lightning-bolts) (21.3)\n",
-      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from lightning-bolts) (0.9.3)\n",
-      "Requirement already satisfied: torch>=1.7.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from lightning-bolts) (1.12.1)\n",
-      "Requirement already satisfied: pyDeprecate>=0.3.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (0.3.2)\n",
-      "Requirement already satisfied: PyYAML>=5.4 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (6.0)\n",
-      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (2022.8.2)\n",
-      "Requirement already satisfied: tensorboard>=2.9.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (2.10.0)\n",
-      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (4.3.0)\n",
-      "Requirement already satisfied: numpy>=1.17.2 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (1.23.3)\n",
-      "Requirement already satisfied: tqdm>=4.57.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pytorch-lightning>=1.4.0->lightning-bolts) (4.64.1)\n",
-      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from packaging->lightning-bolts) (3.0.9)\n",
-      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (3.8.3)\n",
-      "Requirement already satisfied: requests in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (2.28.1)\n",
-      "Requirement already satisfied: setuptools>=41.0.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (57.0.0)\n",
-      "Requirement already satisfied: markdown>=2.6.8 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (3.4.1)\n",
-      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (2.2.2)\n",
-      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (0.6.1)\n",
-      "Requirement already satisfied: grpcio>=1.24.3 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (1.49.1)\n",
-      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (1.8.1)\n",
-      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (0.4.6)\n",
-      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (3.19.5)\n",
-      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (2.11.1)\n",
-      "Requirement already satisfied: wheel>=0.26 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (0.36.2)\n",
-      "Requirement already satisfied: absl-py>=0.4 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (1.2.0)\n",
-      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (1.2.0)\n",
-      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (1.3.1)\n",
-      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (1.8.1)\n",
-      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (4.0.2)\n",
-      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (6.0.2)\n",
-      "Requirement already satisfied: attrs>=17.3.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (22.1.0)\n",
-      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (2.1.1)\n",
-      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (5.2.0)\n",
-      "Requirement already satisfied: six>=1.9.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (1.16.0)\n",
-      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (4.9)\n",
-      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (0.2.8)\n",
-      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (1.3.1)\n",
-      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (4.12.0)\n",
-      "Requirement already satisfied: idna<4,>=2.5 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (3.4)\n",
-      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (1.26.12)\n",
-      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.4.0->lightning-bolts) (2022.9.24)\n",
-      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (2.1.1)\n",
-      "Requirement already satisfied: zipp>=0.5 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (3.8.1)\n",
-      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (0.4.8)\n",
-      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/joonhwi/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning>=1.4.0->lightning-bolts) (3.2.1)\n",
-      "Installing collected packages: lightning-bolts\n",
-      "Successfully installed lightning-bolts-0.5.0\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "#! pip install seaborn\n",
     "#! pip install torchvision\n",
-    "! pip install lightning-bolts"
+    "#! pip install lightning-bolts\n",
+    "#! pip install pytorch_lightning\n",
+    "# install pytorch lighting\n",
+    "! pip install pytorch-lightning --quiet\n",
+    "# install weights and biases\n",
+    "! pip install wandb --quiet\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# 2. Imports"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 14,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "\n",
+    "import pytorch_lightning as pl\n",
+    "# your favorite machine learning tracking tool\n",
+    "from pytorch_lightning.loggers import WandbLogger\n",
+    "from pytorch_lightning import loggers as pl_loggers\n",
+    "\n",
+    "\n",
+    "import torch\n",
+    "from torch import nn\n",
+    "from torch.nn import functional as F\n",
+    "from torch.utils.data import random_split, DataLoader\n",
+    "\n",
+    "from torchmetrics import Accuracy\n",
+    "\n",
+    "from torchvision import transforms\n",
+    "from torchvision.datasets import CIFAR10\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# 3. ë°ì´í„° ëª¨ë“ˆ ì •ì˜í•˜ê¸°\n",
+    "\n",
+    "cifar 10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
+    "\n",
+    "cifar 10 ë°ì´í„° ì…‹ì€ LightningDataModuleì˜ ì„œë¸Œí´ëž˜ìŠ¤ì´ë¯€ë¡œ, ìƒì†ë°›ì•„ ë©”ì„œë“œë¥¼ êµ¬í˜„í•œë‹¤.\n",
+    "\n",
+    "ê³µì‹ doc ì°¸ê³ í•˜ê¸° : https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n",
+    "\n",
+    "### ë©”ì„œë“œ ê°„ëžµ ì„¤ëª…\n",
+    "    prepare_data\n",
+    "GPU í•˜ë‚˜ì—ì„œ í•œ ë²ˆë§Œ í˜¸ì¶œëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì•„ëž˜ì˜ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë‹¨ê³„ì™€ ê°™ë‹¤.\n",
+    "\n",
+    "    setup\n",
+    "ê°ê°ì˜ GPUì—ì„œ ê°œë³„ì ìœ¼ë¡œ í˜¸ì¶œë˜ë©° fit ë˜ëŠ” testë‹¨ê³„ì¼ ê²½ìš° ì •ì˜í•  ìŠ¤í…Œì´ì§€ë¥¼ ë°›ì•„ì˜¨ë‹¤.\n",
+    "\n",
+    "    train_dataloader, val_dataloader, test_dataloader\n",
+    "ê°ê°ì˜ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•œë‹¤.\n",
+    "\n",
+    "### Notes\n",
+    "- random_split : training-validation splitêµ¬ë¶„ì„ ìš©ì´í•˜ê²Œ í•œë‹¤. ì „ì²´ ë°ì´í„°ì…‹ì— ì ìš©ëœë‹¤.\n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "\n",
+    "class CIFAR10DataModule(pl.LightningDataModule):\n",
+    "    def __init__(self, batch_size, data_dir: str = './'):\n",
+    "        super().__init__()\n",
+    "        self.data_dir = data_dir\n",
+    "        self.batch_size = batch_size\n",
+    "\n",
+    "        self.transform = transforms.Compose([\n",
+    "            #í…ì„œí™” ë° ì •ê·œí™”\n",
+    "            transforms.ToTensor(),\n",
+    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
+    "        ])\n",
+    "        \n",
+    "        # í´ëž˜ìŠ¤ ìˆ˜ ì •ì˜\n",
+    "        self.num_classes = 10\n",
+    "    \n",
+    "    def prepare_data(self):\n",
+    "        CIFAR10(self.data_dir, train=True, download=True)\n",
+    "        CIFAR10(self.data_dir, train=False, download=True)\n",
+    "    \n",
+    "    def setup(self, stage=None):\n",
+    "        # Assign train/val datasets for use in dataloaders\n",
+    "        if stage == 'fit' or stage is None:\n",
+    "            #cifar ë°ì´í„° ë°›ì•„ì™€ì„œ ê²½ë¡œ, transform, train, ë“±ë“± ì„¤ì •í•˜ê¸° \n",
+    "            cifar_full = CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
+    "            self.cifar_train, self.cifar_val = random_split(cifar_full, [45000, 5000])\n",
+    "\n",
+    "        # Assign test dataset for use in dataloader(s)\n",
+    "        if stage == 'test' or stage is None:\n",
+    "            self.cifar_test = CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
+    "    \n",
+    "    def train_dataloader(self):\n",
+    "        return DataLoader(self.cifar_train, batch_size=self.batch_size, shuffle=True)\n",
+    "\n",
+    "    def val_dataloader(self):\n",
+    "        return DataLoader(self.cifar_val, batch_size=self.batch_size)\n",
+    "\n",
+    "    def test_dataloader(self):\n",
+    "        return DataLoader(self.cifar_test, batch_size=self.batch_size)\n"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# 2. Initialize "
+    "# 4. Logger ì •ì˜í•˜ê¸°\n",
+    "\n",
+    "epochì´ ëë‚  ë•Œë§ˆë‹¤ ì„±ëŠ¥ ë¡œê·¸ë¥¼ ë‚¨ê¸´ë‹¤. \n",
+    "\n",
+    "## Notes\n",
+    "    pytorch_lightning.callbacks.Callback\n",
+    "\n",
+    "ê³µì‹ docs ë³´ê¸° : https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html\n",
+    "\n",
+    "í”„ë¡œì íŠ¸ ì „ì²´ì—ì„œ ìž¬ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë…ë¦½í˜• í”„ë¡œê·¸ëž¨ì„ ëœ»í•œë‹¤.\n",
+    "\n",
+    "ì˜ˆë¥¼ ë“¤ì–´, í•™ìŠµì¤‘\n",
+    "\n",
+    "    on_validation_epoch_end\n",
+    "\n",
+    "ë©”ì„œë“œê°€ ìˆ˜í–‰ë˜ë©´ callback hook ì´ ìˆ˜í–‰ë˜ì–´ ì•„ëž˜ì˜ ì½”ë“œê°€ ìˆ˜í–‰ëœë‹¤.\n",
+    "\n",
+    "ë‹¤ì‹œë§í•´ validation epochì´ ì¢…ë£Œë  ë•Œ, ë¡œê·¸ë¥¼ ë‚¨ê¸°ëŠ” ì½”ë“œê°€ ìˆ˜í–‰ë˜ëŠ” ê²ƒì´ë‹¤."
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 24,
+   "execution_count": 16,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "class ImagePredictionLogger(pl.callbacks.Callback):\n",
+    "    def __init__(self, val_samples, num_samples=32):\n",
+    "        super().__init__()\n",
+    "        self.num_samples = num_samples\n",
+    "        self.val_imgs, self.val_labels = val_samples\n",
+    "    \n",
+    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
+    "        # Bring the tensors to CPU\n",
+    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
+    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
+    "        # Get model prediction\n",
+    "        logits = pl_module(val_imgs)\n",
+    "        preds = torch.argmax(logits, -1)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# 5. ëª¨ë¸ ì •ì˜í•˜ê¸°\n",
+    "\n",
+    "LightningModuleì€ ëª¨ë¸ì´ ì•„ë‹Œ ì‹œìŠ¤í…œì„ ì •ì˜í•˜ê¸° ë•Œë¬¸ì—, ëª¨ë¸ì„ ë‹¨ì¼ í´ëž˜ìŠ¤ë¡œ ë…ë¦½í™”ì‹œì¼œì•¼í•œë‹¤.\n",
+    "\n",
+    "ê¸°ì¡´ì˜ Pytorch ì½”ë“œë¥¼ 5ê°œì˜ ì„¹ì…˜ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ë„£ì–´ì£¼ë©´ ëœë‹¤.\n",
+    "- Computations (__init__).\n",
+    "\n",
+    "- Train loop (training_step)\n",
+    "\n",
+    "- Validation loop (validation_step)\n",
+    "\n",
+    "- Test loop (test_step)\n",
+    "\n",
+    "- Optimizers (configure_optimizers)\n",
+    "\n",
+    "### ë©”ì„œë“œ ê°„ëžµ ì„¤ëª…\n",
+    "pytorch_lightning.LightningModuleì€ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜ì™€ forward ì „ë‹¬ ë°©ì‹ì„ ìƒì†ë°›ì•„ êµ¬í˜„í•  ìˆ˜ ìžˆê²Œ í•´ë†¨ë‹¤.\n",
+    "\n",
+    "#### 1. init ë©”ì„œë“œ\n",
+    "\n",
+    "initì—ì„œ ëª¨ë¸ì— í•„ìš”í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•œë‹¤.\n",
+    "\n",
+    "    save_parameters\n",
+    "\n",
+    "ë¥¼ call í•¨ìœ¼ë¡œì¨ initì— ìžˆëŠ” ëª¨ë“  ê°’ì„ check pointì— ì €ìž¥í•˜ë„ë¡ ìš”ì²­í•  ìˆ˜ ìžˆë‹¤.\n",
+    "\n",
+    ". . .\n",
+    "\n",
+    "\n",
+    "    _get_conv_outputê³¼ _forward_features\n",
+    "\n",
+    "ë©”ì„œë“œëŠ” convolutional blockì˜ í…ì„œ ì‚¬ì´ì¦ˆë¥¼ ìžë™ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. \n",
+    "\n",
+    ". . .\n",
+    "\n",
+    "    forward\n",
+    "ì¼ë‹¨ íŒŒì´í† ì¹˜ ì½”ë“œì™€ ë¹„ìŠ·í•˜ë‚˜, ë¼ì´íŠ¸ë‹ì—ì„œëŠ” ì˜¤ì§ inference actionì„ ìœ„í•´ì„œë§Œ ì‚¬ìš©ëœë‹¤. training_stepì€ í•™ìŠµ loopë¥¼ ì •ì˜í•œë‹¤.\n",
+    "\n",
+    "\n",
+    "#### 2. Training step ë©”ì„œë“œ\n",
+    "\n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "class LitModel(pl.LightningModule):\n",
+    "    def __init__(self, input_shape, num_classes, learning_rate=2e-4):\n",
+    "        super().__init__()\n",
+    "        \n",
+    "        # log hyperparameters\n",
+    "        self.save_hyperparameters()\n",
+    "        self.learning_rate = learning_rate\n",
+    "        \n",
+    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
+    "        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n",
+    "        self.conv3 = nn.Conv2d(32, 64, 3, 1)\n",
+    "        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n",
+    "\n",
+    "        self.pool1 = torch.nn.MaxPool2d(2)\n",
+    "        self.pool2 = torch.nn.MaxPool2d(2)\n",
+    "        \n",
+    "        n_sizes = self._get_conv_output(input_shape)\n",
+    "\n",
+    "        self.fc1 = nn.Linear(n_sizes, 512)\n",
+    "        self.fc2 = nn.Linear(512, 128)\n",
+    "        self.fc3 = nn.Linear(128, num_classes)\n",
+    "\n",
+    "        self.accuracy = Accuracy()\n",
+    "\n",
+    "    # returns the size of the output tensor going into Linear layer from the conv block.\n",
+    "    def _get_conv_output(self, shape):\n",
+    "        batch_size = 1\n",
+    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
+    "\n",
+    "        output_feat = self._forward_features(input) \n",
+    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
+    "        return n_size\n",
+    "        \n",
+    "    # returns the feature tensor from the conv block\n",
+    "    def _forward_features(self, x):\n",
+    "        x = F.relu(self.conv1(x))\n",
+    "        x = self.pool1(F.relu(self.conv2(x)))\n",
+    "        x = F.relu(self.conv3(x))\n",
+    "        x = self.pool2(F.relu(self.conv4(x)))\n",
+    "        return x\n",
+    "    \n",
+    "    # will be used during inference\n",
+    "    def forward(self, x):\n",
+    "       x = self._forward_features(x)\n",
+    "       x = x.view(x.size(0), -1)\n",
+    "       x = F.relu(self.fc1(x))\n",
+    "       x = F.relu(self.fc2(x))\n",
+    "       x = F.log_softmax(self.fc3(x), dim=1)\n",
+    "       \n",
+    "       return x\n",
+    "    \n",
+    "    def training_step(self, batch, batch_idx):\n",
+    "        x, y = batch\n",
+    "        logits = self(x)\n",
+    "        loss = F.nll_loss(logits, y)\n",
+    "        \n",
+    "        # training metrics\n",
+    "        preds = torch.argmax(logits, dim=1)\n",
+    "        acc = self.accuracy(preds, y)\n",
+    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
+    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
+    "        \n",
+    "        return loss\n",
+    "    \n",
+    "    def validation_step(self, batch, batch_idx):\n",
+    "        x, y = batch\n",
+    "        logits = self(x)\n",
+    "        loss = F.nll_loss(logits, y)\n",
+    "\n",
+    "        # validation metrics\n",
+    "        preds = torch.argmax(logits, dim=1)\n",
+    "        acc = self.accuracy(preds, y)\n",
+    "        self.log('val_loss', loss, prog_bar=True)\n",
+    "        self.log('val_acc', acc, prog_bar=True)\n",
+    "        return loss\n",
+    "    \n",
+    "    def test_step(self, batch, batch_idx):\n",
+    "        x, y = batch\n",
+    "        logits = self(x)\n",
+    "        loss = F.nll_loss(logits, y)\n",
+    "        \n",
+    "        # validation metrics\n",
+    "        preds = torch.argmax(logits, dim=1)\n",
+    "        acc = self.accuracy(preds, y)\n",
+    "        self.log('test_loss', loss, prog_bar=True)\n",
+    "        self.log('test_acc', acc, prog_bar=True)\n",
+    "        return loss\n",
+    "    \n",
+    "    def configure_optimizers(self):\n",
+    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
+    "        return optimizer"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 18,
    "metadata": {},
    "outputs": [
     {
-     "name": "stderr",
+     "name": "stdout",
      "output_type": "stream",
      "text": [
-      "/var/folders/9y/5kfy0x_12x925thrsg7zxykr0000gn/T/ipykernel_18771/3278709481.py:9: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
-      "  from IPython.core.display import display\n",
-      "Global seed set to 7\n"
+      "Files already downloaded and verified\n",
+      "Files already downloaded and verified\n"
      ]
     },
     {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      ".\n"
-     ]
+     "data": {
+      "text/plain": [
+       "(torch.Size([32, 3, 32, 32]), torch.Size([32]))"
+      ]
+     },
+     "execution_count": 18,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
-    "import os\n",
+    "dm = CIFAR10DataModule(batch_size=32)\n",
+    "# To access the x_dataloader we need to call prepare_data and setup.\n",
+    "dm.prepare_data()\n",
+    "dm.setup()\n",
     "\n",
-    "import pandas as pd\n",
-    "import seaborn as sn\n",
-    "import torch\n",
-    "import torch.nn as nn\n",
-    "import torch.nn.functional as F\n",
-    "import torchvision\n",
-    "from IPython.core.display import display\n",
-    "from pl_bolts.datamodules import CIFAR10DataModule\n",
-    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
-    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
-    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
-    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
-    "from pytorch_lightning.loggers import CSVLogger\n",
-    "from torch.optim.lr_scheduler import OneCycleLR\n",
-    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
-    "from torchmetrics.functional import accuracy\n",
-    "\n",
-    "seed_everything(7)\n",
-    "\n",
-    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
-    "print(PATH_DATASETS)\n",
-    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
-    "NUM_WORKERS = int(os.cpu_count() / 2)\n"
+    "# Samples required by the custom ImagePredictionLogger callback to log image predictions.\n",
+    "val_samples = next(iter(dm.val_dataloader()))\n",
+    "val_imgs, val_labels = val_samples[0], val_samples[1]\n",
+    "val_imgs.shape, val_labels.shape"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 25,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [
     {
-     "ename": "TypeError",
-     "evalue": "__init__() got an unexpected keyword argument 'train_transforms'",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
-      "Cell \u001b[0;32mIn [25], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m train_transforms \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mRandomCrop(\u001b[38;5;241m32\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     ]\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m test_transforms \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     11\u001b[0m     [\n\u001b[1;32m     12\u001b[0m         torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     13\u001b[0m         cifar10_normalization(),\n\u001b[1;32m     14\u001b[0m     ]\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m cifar10_dm \u001b[38;5;241m=\u001b[39m \u001b[43mCIFAR10DataModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATH_DATASETS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_transforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_transforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_transforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_transforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
-      "File \u001b[0;32m~/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages/pl_bolts/datamodules/cifar10_datamodule.py:89\u001b[0m, in \u001b[0;36mCIFAR10DataModule.__init__\u001b[0;34m(self, data_dir, val_split, num_workers, normalize, batch_size, seed, shuffle, pin_memory, drop_last, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     64\u001b[0m     data_dir: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     75\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39m        data_dir: Where to save/load the data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39m        drop_last: If true drops the last incomplete batch\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(  \u001b[39m# type: ignore[misc]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m         data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m     91\u001b[0m         val_split\u001b[39m=\u001b[39;49mval_split,\n\u001b[1;32m     92\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m     93\u001b[0m         normalize\u001b[39m=\u001b[39;49mnormalize,\n\u001b[1;32m     94\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     95\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     96\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m     97\u001b[0m         pin_memory\u001b[39m=\u001b[39;49mpin_memory,\n\u001b[1;32m     98\u001b[0m         drop_last\u001b[39m=\u001b[39;49mdrop_last,\n\u001b[1;32m     99\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    100\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    101\u001b[0m     )\n",
-      "File \u001b[0;32m~/Desktop/KAU/4-2/capstone/crack_segmentaion/venv/lib/python3.8/site-packages/pl_bolts/datamodules/vision_datamodule.py:47\u001b[0m, in \u001b[0;36mVisionDataModule.__init__\u001b[0;34m(self, data_dir, val_split, num_workers, normalize, batch_size, seed, shuffle, pin_memory, drop_last, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     20\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     21\u001b[0m     data_dir: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     32\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m        data_dir: Where to save/load the data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m        drop_last: If true drops the last incomplete batch\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir \u001b[39m=\u001b[39m data_dir \u001b[39mif\u001b[39;00m data_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m os\u001b[39m.\u001b[39mgetcwd()\n\u001b[1;32m     50\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_split \u001b[39m=\u001b[39m val_split\n",
-      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'train_transforms'"
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
      ]
     }
    ],
    "source": [
+    "model = LitModel((3, 32, 32), dm.num_classes)\n",
+    "\n",
+    "# Initialize wandb logger\n",
+    "wandb_logger = WandbLogger(project='wandb-lightning', job_type='train')\n",
+    "\n",
+    "# Initialize logger\n",
+    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\n",
     "\n",
-    "train_transforms = torchvision.transforms.Compose(\n",
-    "    [\n",
-    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
-    "        torchvision.transforms.RandomHorizontalFlip(),\n",
-    "        torchvision.transforms.ToTensor(),\n",
-    "        cifar10_normalization(),\n",
-    "    ]\n",
-    ")\n",
-    "\n",
-    "test_transforms = torchvision.transforms.Compose(\n",
-    "    [\n",
-    "        torchvision.transforms.ToTensor(),\n",
-    "        cifar10_normalization(),\n",
-    "    ]\n",
-    ")\n",
-    "\n",
-    "cifar10_dm = CIFAR10DataModule(\n",
-    "    data_dir=PATH_DATASETS,\n",
-    "    batch_size=BATCH_SIZE,\n",
-    "    num_workers=NUM_WORKERS,\n",
-    "    train_transforms=train_transforms,\n",
-    "    test_transforms=test_transforms,\n",
-    "    val_transforms=test_transforms,\n",
-    ")"
+    "# Initialize Callbacks\n",
+    "early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\")\n",
+    "checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
+    "\n",
+    "# Initialize a trainer\n",
+    "trainer = pl.Trainer(max_epochs=10,\n",
+    "                     gpus=0, \n",
+    "                     logger=tb_logger,\n",
+    "                     callbacks=[early_stop_callback,\n",
+    "                                ImagePredictionLogger(val_samples),\n",
+    "                                checkpoint_callback],\n",
+    "                     )\n",
+    "\n",
+    "# Train the model âš¡ðŸš…âš¡\n",
+    "trainer.fit(model, dm)\n",
+    "\n",
+    "# Evaluate the model on the held-out test set âš¡âš¡\n",
+    "trainer.test(dataloaders=dm.test_dataloader())"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
   }
  ],
  "metadata": {
